---
title: "Kernel Methods"
author: "Héctor Corrada Bravo"
company: "University of Maryland"
date: "`r Sys.Date()`"
css: ["custom.css"]
output: 
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    includes:
      after_body: "custom.html"
    nature:
      ratio: "16:9"
---

class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Kernel Methods]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC 643: `r Sys.Date()`
]

.logo[![](img/logo.png)]

---

$$
\mathrm{min}_{b, \mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t.} y_i(b + \mathbf{w}'x_i) \geq 1 \, \forall i
$$

This is a _constrained_ optimization problem

Minimize the norm of $\mathbf{w}$ under the constraint that it classifies every observation correctly. 

---
exclude: true

We can switch between equivalent constrained minimization and constrained maximization problems. 

In the maximum-margin hyper-plane case, the equivalent constrained maximization problem (the _dual_ problem) is:

$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \alpha_i \geq 0 \, \forall i$$

---
exclude: true

## Maximum margin hyper-planes

$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \alpha_i \geq 0 \, \forall i$$

This quadratic optimization problem is usually easier to optimize than the original problem (notice there is only positivity constraints on $\alpha$).

---
exclude: true

## Support Vector Machines

_Key insight no. 2_: **SVMs only depend on pairwise "similarity" functions of observations**

$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \alpha_i = 0 \, \forall i$$

Only inner products between observations are required as opposed to the observations themselves.

---
exclude: true

## Support Vector Machines


Also, we can write the _discriminant_ function in equivalent form

$$f(x) = \beta_0 + \sum_{i=1}^n y_i \alpha_i x'x_i$$



Geometrically, we can think of the inner product between observations as a "similarity" measure. 



Therefore, we can fit these models with other measures that works as "similarities". 

---
exclude: true

## Support Vector Machines

_Key insight no. 3_: **SVMs only depend on a subset of observations (support vectors)**

Optimial solutions $\beta$ and $\alpha$ must satisfy the following condition:

$$
\alpha_i [ y_i(\beta_0 + \beta'x_i) -1] = 0 \, \forall i.
$$

---
exclude: true

## Support Vector Machines

$$
\alpha_i [ y_i(\beta_0 + \beta'x_i) -1] = 0 \, \forall i.
$$

Case 1: $\alpha_i > 0$, then the signed distance between observation $x_i$ and the decision boundary is 1. 

This means that observation $x_i$ is _on the margin_

---
exclude: true

## Support Vector Machines

$$
\alpha_i [ y_i(\beta_0 + \beta'x_i) -1] = 0 \, \forall i.
$$

Case 2: $y_i (\beta_0 + \beta'x_i) > 1$, then observation $x_i$ is not on the margin and $\alpha_i=0$.

---
exclude: true

## Support Vector Machines

To define the discriminant function in terms of $\alpha$s we only need observations that are _on the margin_, 

i.e., those for which $\alpha_i > 0$. 



These are called _support vectors_. 



Also implies we only need Support Vectors to make predictions.

---

## Kernel Methods

We saw predictive models that depend only on inner products

and that this can be generalized using the "kernel trick"

--

Effects:

1) a similar "trick" can be used for methods other than SVMs, 

2) kernels are capable of representing datatypes that are more complex than the tabular representation we have been using so far.

---
exclude: true

## Non-separable data

An elegant result is that this formulation doesn't change the dual problem we saw before very much:

$$\mathrm{max}_{\alpha} \; \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \; 0 \leq \alpha_i \leq C \, \forall i$$

---
exclude: true

## Non-separable data

Only need support vectors, where $\alpha_i > 0$ to define the discriminant function and make predictions. 


The smaller the cost parameter $C$, the learned SVM will have fewer support vectors. 


Think of the number of support vectors as a rough measure of the _complexity_ of the SVM obtained. 

---
exclude: true
class: split-50

## Non-linear Support Vector Machine

.column[What to do when we need non-linear partitions of predictor space to get a classifier?]

.column[
![](images/9_8.png)
]

---
exclude: true

## Non-linear Support Vector Machine

We can define the SVM discriminant function in terms of inner products of observations. 

We can generalize inner product using "kernel" functions that provide something like an inner product:

$$f(x) = \beta_0 + \sum_{i=1}^n y_i \alpha_i k(x, x_i)$$


---
exclude: true
class: split-50

## Non-linear Support Vector Machine

.column[But, what is $k$? Let's consider two examples.

- _Polynomial kernel_: $k(x,x_i) = 1+\langle x, x_i \rangle^d$

- _RBF (radial) kernel_: $k(x,x_i) = \exp\{-\gamma \sum_{j=1}^p (x_{j}-x_{ij})^2\}$
]

.column[![](images/9_9.png)]

---
exclude: true

## Non-linear Support Vector Machine

```{r, echo=FALSE, fig.align='center'}
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))

k <- function(x, x0=0, gamma=1) {
  exp(-gamma*(x-x0)^2)
}
x <- seq(-3, 3, len=100)
plot(x, k(x), type="l", lwd=2, col=1, main="RBF kernel")
lines(x, k(x,gamma=10), lwd=2, col=2)
lines(x, k(x,gamma=.1), lwd=2, col=3)
legend("topright", legend=paste("gamma=",c(1,10,.1)), lty=1, lwd=2, col=1:3)
```

---
exclude: true

## Non-linear Support Vector Machine

The optimization problem is very similar

$$\mathrm{max}_{\alpha} \; \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k k(x_i,x_k) \\\\
\mathrm{s.t.} \; 0 \leq \alpha_i \leq C \, \forall i$$

---
---
exclude: true

## SVM classification example

Let's try fitting SVMs to the credit card default dataset we saw in previous examples. 

Let's start with a linear SVM (where $k$ is the inner product). 

---
exclude: true

## SVM classification example

Here we are fitting three different SVMs resulting from using three different values of cost parameter $C$.

```{r ch06_fitsvm, cache=TRUE, echo=FALSE}
library(e1071)
library(ISLR)
data(Default)

n <- nrow(Default)
train_indices <- sample(n, n/2)

costs <- c(.01, 1, 100)
svm_fits <- lapply(costs, function(cost) {
  svm(default~., data=Default, cost=cost, kernel="linear",subset=train_indices)
})
```

```{r, echo=FALSE}
number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Default[train_indices,])
  train <- mean(yhat != Default$default[train_indices])
  yhat <- predict(fit, newdata=Default[-train_indices,])
  test <- mean(yhat != Default$default[-train_indices])
  c(train=train, test=test)
})

tab <- data.frame(cost=costs, number_svs=number_svs, train_error=error_rate["train",]*100,test_error=error_rate["test",]*100)
knitr::kable(tab, format="html")
```

---
exclude: true

## SVM classification example

Let's try now a _non-linear_ SVM by using a radial kernel. 

Notice now that we have two parameters to provide to the fitting function: cost parameter $C$ and parameter $\gamma$ of the radial kernel function.

---
exclude: true

## SVM classification example

```{r ch06_fitradialsvm, cache=TRUE, echo=FALSE}
costs <- c(.01, 1, 10)
gamma <- c(.01, 1, 10)
parameters <- expand.grid(costs, gamma)

svm_fits <- lapply(seq(nrow(parameters)), function(i) {
  svm(default~., data=Default, cost=parameters[i,1], kernel="radial", gamma=parameters[i,2], subset=train_indices)
})
```


```{r, echo=FALSE}
number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Default[train_indices,])
  train <- mean(yhat != Default$default[train_indices])
  yhat <- predict(fit, newdata=Default[-train_indices,])
  test <- mean(yhat != Default$default[-train_indices])
  c(train=train, test=test)
})

tab <- data.frame(cost=parameters[,1], gamma=parameters[,2], number_svs=number_svs, train_error=error_rate["train",]*100,test_error=error_rate["test",]*100)
knitr::kable(tab, format="html")
```

## The SVM as a regularized estimation method

Consider the linear SVM formulation again.

$$\mathrm{min}_{\beta_0,\beta,\xi} \; C\sum_{i=1}^N \xi_i + \frac{1}{2} \|\beta\|^2 \\\\
\mathrm{s.t} \; y_i(\beta_0 + \beta'x_i) \geq 1 - \xi_i \, \forall i \\\\
\xi_i \geq 0 \, \forall i$$

---

## The SVM as a regularized estimation method

This is equivalent to the following optimization problem with $\lambda = 1/C$

$$\mathrm{min}_{\beta_0, \beta} \; \sum_{i=1}^N (1-y_if_i)_+ + \frac{\lambda}{2} \|\beta\|^2$$

- $f_i = \beta_0 + \beta'x_i$
- $(1-y_if_i)_+$ denoting the _positive part_ of $1-y_if_i$. 

---

## The SVM as a regularized estimation method

$$\mathrm{min}_{\beta_0, \beta} \; \sum_{i=1}^N (1-y_if_i)_+ + \frac{\lambda}{2} \|\beta\|^2$$

If observation $x_i$ is on the proper side of the margin, 

then $y_if_i > 1$ and thus $(1-y_ifi)_+ = 0$. 

--

Otherwise, $(1-y_if_i)_+$ equals the signed distance to the margin for observation $x_i$.

---

## The SVM as a regularized estimation method

$$\mathrm{min}_{\beta_0, \beta} \; \sum_{i=1}^N (1-y_if_i)_+ + \frac{\lambda}{2} \|\beta\|^2$$

This formulation makes the connection to SVMs as regularized estimation procedure much clearer. 

--

The first term corresponds to a "loss function"

--

The second term a regularization term that controls model complexity.

---

## The SVM as a regularized estimation method

For the non-linear SVM, recall the discriminant function has form

$$f(x) = \beta_0 + \sum_{i=1}^N \alpha_i y_i k(x_i, x)$$

---

## The SVM as a regularized estimation method

The optimization problem takes a similar form:

$$\mathrm{min}_{\alpha_0, \alpha} \; \sum_{i=1}^N (1-y_if_i)_+ + \frac{\lambda}{2} \alpha'K\alpha$$

where $K$ is the $N \times N$ matrix with $y_i y_j K_{ij}=k(x_i,x_j)$.

---
class: split-40

## Kernelized logistic regression

.column[The loss function in the first term of the formulation is called "Hinge-loss".

We can compare it with the likelihood function for logistic regression.] 


.column[
```{r, echo=FALSE, fig.width=6, fig.height=5, fig.align="center"}
library(RColorBrewer)

palette(brewer.pal(3, "Dark2"))

curve(pmax(1-x,0), from=-3, to=3, lwd=1.8, col=1, xlab=expression(y*f), ylab="Loss")
curve(log(1+exp(-x)), add=TRUE, lwd=1.8, col=2)
abline(h=0, lwd=1.5, lty=2)
legend("topright", legend=c("Hinge Loss", "Logistic"), col=1:2, lwd=1.5)
```
]

---

## Kernelized Logistic Regression

We can therefore use the same "loss + penalty" formulation to obtain a kernelized version of logistic regression:

$$\mathrm{min}_{\beta_0, \beta} \; \sum_{i=1}^N \log(1+e^{-y_if_i}) + \frac{\lambda}{2} \alpha'K\alpha$$

---

## Kernelized Logistic Regression

As before, function $f$ has a linear expansion in terms of the kernel function:

$$f(x) = \beta_0 + \sum_{i=1}^N \alpha_i k(x,x_i)$$

--

Unlike the SVM, the logistic regression loss function does not tend to set $\alpha_i=0$ for correctly classified observations.

---

## Kernelized Logistic Regression

As before, function $f$ has a linear expansion in terms of the kernel function:

$$f(x) = \beta_0 + \sum_{i=1}^N \alpha_i k(x,x_i)$$

Nonetheless, function $f$ retains the interpretation in logistic regression

$$f(x) = \frac{Pr(Y = +1| X=x)}{Pr(Y = -1 | X=x)}$$

---

## Kernelized Regression

In similar fashion, we could build non-linear regression models using the "kernel trick" by using least squares as the loss function when predicting continuous outcomes.

$$\min{\alpha_0, \alpha} \; \sum_{i=1}^N (y_i - f_i)^2 + \frac{\lambda}{2} \alpha'K\alpha$$

---

## Kernelized Regression

Again, function $f$ has a linear expansion in terms of the kernel function

$$f(x) = \beta_0 + \sum_{i=1}^N \alpha_i k(x_i, x)$$

and retains the interpretation as conditional expection

$$f(x) = E[Y | X=x]$$

---

## Kernelized Regression

This does not lead to sparse representations over a subset of observations like SVMS.

However, a different choice of loss function, similar to hinge loss, can lead to sparse representations.

---

## Support Vector Regression

Support Vector Regression refers to the "loss + penalty" formulation when $\epsilon$-insensitive loss is used:

$$V_{\epsilon}(r) =
\begin{cases}
0 & \mathrm{if } |r| < \epsilon \\
|r| - \epsilon & \mathrm{otherwise}
\end{cases}$$

---
class: split-60
## Support Vector Regression

.column[We can compare $\epsilon$-insensitive loss to squared loss]

.column[
```{r, echo=FALSE, fig.height=5, fig.width=5}
curve(x^2, from=-4, to=4, col=2, lwd=2, xlab=expression(r), ylab="Loss", ylim=c(0,6))
curve(pmax(0,abs(x)-1.5), add=TRUE, lwd=2, col=1)
abline(h=0, lwd=1.5, lty=2)
legend("topright", legend=c(expression(paste(epsilon,"-insensitive")), "squared"), col=1:2, lty=1, lwd=1.5)
```
]

---
class: split-40

## Modeling labeled data sequences

.column[Consider the case where predictors for observations are structured as sequences. 

For instance, predictors correspond to some variable measured over time.]

.column[
![](images/ts.png)
]

---
class: split-40

## Modeling labeled data sequences

.column[In this case, each observation is represented by a time series

we want to discriminate between time series that belong to two different classes.]

.column[
![](images/ts.png)
]

---

## Modeling labeled data sequences

Using the results above, we could model this using a Support Vector Machine, providing a kernel that captures similarity between time series.

Some proposals for this include:

- Autoregressive kernels: Cuturi, Doucet (2011) https://arxiv.org/abs/1101.0673. 

The likelihod of a vector autoregressive model is used to create a similarity metric.

---

## Modeling labeled data sequences

Using the results above, we could model this using a Support Vector Machine, providing a kernel that captures similarity between time series.

Some proposals for this include:

- Dynamic Time Warping Kernel: Shimodaira (2002) https://papers.nips.cc/paper/2131-dynamic-time-alignment-kernel-in-support-vector-machine.pdf. 

A warping method is used to define distances between data series

---
class: split-40

## Modeling labeled data sequences

.column[- Reservoir Computing: Chen et al. http://dl.acm.org/citation.cfm?id=2487700. 

Reservoir state models are used to represent time series and derive kernels
]

.column[.image-90[![](images/reservoir_ts.png)]]

---

## Structured Output

The "loss + penalty" representation also allows additional flexibility in the types of outcomes that are predicted. 

For instance, consider the case where outcomes are numerical vectors 

$\mathbf{y_i}=(y_{i1},y_{i2},\ldots,y_{iT})$ for each observation

along with predictors $x_i$ as before. 

---

## Structured Output

In this case, we would use function $f$ to represent a vector as well:

$$f(x) = 
\left( 
\begin{aligned}
\alpha_{01} + \sum_{i=1}^N \alpha_{i1}k(x_i,x) \\\\
\alpha_{02} + \sum_{i=1}^N \alpha_{i2}k(x_i,x) \\\\
\vdots \\\\
\alpha_{0T} + \sum_{i=1}^N \alpha_{iT}k(x_i,x)
\end{aligned}
\right)$$

---

## Structured Output

We can then use a matrix norm on residuals as a loss function

$$\min_{\alpha_0, \alpha} \sum_{i=1}^N \|R\|_F^2 + \frac{1}{2} \sum_{j=1}^T \alpha_j' K \alpha_j$$

- column $i$ of residual matrix $R_i=y_i - f(x_i)$ 

- $\|R\|_F^2 = R'R$ is the Frobenius norm of residual matrix $R$

- $\alpha_j$ is the vector of parameters for component $j$ of the model.

---

## Structured Output

This regression model is over-parameterized 

Is difficult to estimate

Does not capture any underlying structure in the outcome vector $Y$.

---

## Structured Output

Methods to address this shortcoming are generalized under the term "structured output learning". 

A good starting point is https://mitpress.mit.edu/books/predicting-structured-data

---

## Summary

The general "loss + penalty" formulation along with kernel methods are capable of capturing a wide array of learning applications.

--

A number of effective methods to represent similarities between data series, e.g., time series, as kernel functions allows the usage of this framework to those types of problems.

--

Structured output formulations are applicable to learn multivariate outcomes with dependency structure between the components of the outcomes.

