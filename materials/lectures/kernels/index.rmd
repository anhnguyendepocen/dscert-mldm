---
title: "Kernel Methods"
author: "Héctor Corrada Bravo"
company: "University of Maryland"
date: "`r Sys.Date()`"
css: ["custom.css"]
output: 
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    includes:
      after_body: "custom.html"
    nature:
      ratio: "16:9"
---

class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Kernel Methods]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC 643: `r Sys.Date()`
]

.logo[![](img/logo.png)]

---
layout: true

## Support Vector Machines

---

Let's recall the SVM optimization problem

$$
\mathrm{min}_{b, \mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t.} y_i(b + \mathbf{w}'x_i) \geq 1 \, \forall i
$$

This is a _constrained_ optimization problem

Minimize the norm of $\mathbf{w}$ under the constraint that it classifies every observation correctly (on the proper side of the _margin_). 

---


We can switch between equivalent constrained minimization and constrained maximization problems. 

In the maximum-margin hyper-plane case, the equivalent constrained maximization problem (the _dual_ problem) is:

$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \alpha_i \geq 0 \, \forall i$$

---


$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \alpha_i \geq 0 \, \forall i$$

This quadratic optimization problem is usually easier to optimize than the original problem (notice there is only positivity constraints on $\alpha$).

We can use Projected Gradient Descent, where after each step we ensure that all $\alpha_i \geq 0$ by setting any $\alpha_i < 0$ to 0.

---

An important result is then that

_Key insight_: **SVMs only depend on pairwise "similarity" functions of observations**

$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \alpha_i = 0 \, \forall i$$

Only inner products between observations are required as opposed to the observations themselves.

---

Also, we can write the _discriminant_ function in equivalent form

$$f(x) = b + \sum_{i=1}^n y_i \alpha_i x'x_i$$



Geometrically, we can think of the inner product between observations as a "similarity" measure. 

Therefore, we can fit these models with other measures that works as "similarities". 

---

This leads to another important point:

_Key insight_: **SVMs only depend on a subset of observations (support vectors)**

Optimal solutions $b$, $\mathbf{w}$ and $\alpha$ must satisfy the following condition:

$$
\alpha_i [ y_i(b + \mathbf{w}'x_i) -1] = 0 \, \forall i.
$$

---

$$
\alpha_i [ y_i(b + \mathbf{w}'x_i) -1] = 0 \, \forall i.
$$

Case 1: $\alpha_i > 0$, then the signed distance between observation $x_i$ and the decision boundary is 1. 

This means that observation $x_i$ is _on the margin_

---

$$
\alpha_i [ y_i(b + \mathbf{w}'x_i) -1] = 0 \, \forall i.
$$

Case 2: $y_i (b + \mathbf{w}'x_i) > 1$, then observation $x_i$ is not on the margin and $\alpha_i=0$.

---


To define the discriminant function in terms of $\alpha$s we only need observations that are _on the margin_, 

i.e., those for which $\alpha_i > 0$. 

These are called _support vectors_. 

Also implies we only need Support Vectors to make predictions.

---
exclude: true

## Kernel Methods

We saw predictive models that depend only on inner products

and that this can be generalized using the "kernel trick"

Effects:

1) a similar "trick" can be used for methods other than SVMs, 

2) kernels are capable of representing datatypes that are more complex than the tabular representation we have been using so far.

---
layout: true

## Non-separable data

---

Let's review the SVM problem for non-separable data:

$$\mathrm{min}_{b,\mathbf{w},\xi} \; \sum_{i=1}^N \xi_i + \frac{\lambda}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t} \; y_i(b + \mathbf{w}'x_i) \geq 1 - \xi_i \, \forall i \\\\
\xi_i \geq 0 \, \forall i$$

---

An elegant result is that this formulation doesn't change the dual problem we saw before very much:

$$\mathrm{max}_{\alpha} \; \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \; 0 \leq \alpha_i \leq \frac{1}{\lambda} \, \forall i$$

---

Only need support vectors, where $\alpha_i > 0$ to define the discriminant function and make predictions. 


The larger the penalty parameter $\lambda$, the learned SVM will have fewer support vectors. 


Think of the number of support vectors as a rough measure of the _complexity_ of the SVM obtained. 

---
layout: true
class: split-50

## Non-linear Support Vector Machine

.column[
![](images/9_8.png)
]

---

.column[What to do when we need non-linear partitions of predictor space to get a classifier?]

---

.column[Two options:

Construct non-linear features from original features

Kernel methods (in a second)
]

---
layout: true

## Tangent: Feature Construction

---
class: split-50

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(cache=TRUE)
library(png)
library(grid)
library(tidyr)
library(dplyr)
```


### Centering and scaling

.column[Given data $x = x_1, x_2, \ldots, x_n$, the transformation applied to obtain centered and scaled variable $z$ is:

$$
z_i = \frac{(x_i - \overline{x})}{\mathrm{sd}(x)}
$$

where $\overline{x}$ is the mean of data $x$, and $\mathrm{sd}(x)$ is its standard deviation.
]

.column[
```{r, echo=FALSE, fig.width=5}
library(ggplot2)
data(diamonds)

diamonds %>%
  mutate(scaled_depth = (depth - mean(depth)) / sd(depth)) %>%
  ggplot(aes(x=scaled_depth)) +
    geom_histogram(binwidth=.5)
```
]

---

Another name for this transformation is to _standardize_ a variable.

Quiz: What is the mean of $z$? What is it's standard deviation? 

$$
z_i = \frac{(x_i - \overline{x})}{\mathrm{sd}(x)}
$$

---

One useful result of applying this transformation to variables in a dataset is that all variables are in the same, and thus comparable units.

On occasion, you will have use to apply transformations that only _center_ (but not scale) data:

$$
z_i = (x_i - \overline{x})
$$

Quiz: what is the mean of $z$ in this case? What is it's standard deviation?

---

Or, apply transformations that only _scale_ (but not center) data:

$$
z_i = \frac{x_i}{\mathrm{sd}(x_i)}
$$

Question: what is the mean of $z$ in this case? What is it's standard deviation?


---
class: split-40

### Skewed Data

.column[
In many data analysis, variables will have a _skewed_ distribution over their range. 

Applying a transformation to reduce skew can improve prediction performance.
]

.column[
```{r echo=FALSE}
library(dplyr)
library(ggplot2)
library(nycflights13)
```

```{r, echo=FALSE, fig.width=5, warning=FALSE}
flights %>% ggplot(aes(x=dep_delay)) + geom_histogram(binwidth=30)
```
]

---
class: split-40

## Skewed data

.column[
Logarithmic transform

If some values are negative, two options

Started Log: shift all values so they are positive, apply `log2`
Signed Log: $sign(x) \times log2(abs(x) + 1)$.
]
 
.column[ 
```{r, echo=FALSE, warning=FALSE, fig.width=5}
flights %>%
  mutate(transformed_dep_delay = sign(dep_delay) * log2(abs(dep_delay) + 1)) %>%
  ggplot(aes(x=transformed_dep_delay)) +
    geom_histogram(binwidth=1)
```
]

---

## Non-linear Transforms

Besides these "normalizing" transformations, we can construct features to induce non-linearity in our models.

Polynomial transformations: given feature $x_j$, use features $x_j,x^2_j,x^3_j$ in linear model

Interaction features: combine features using products in linear model, e.g., include feature $x_jx_{j'}$, etc.

---
layout: false

## Non-linear Support Vector Machine

Kernel Methods provide a different way of doing this.

We can define the SVM discriminant function in terms of inner products of observations. 

We can generalize inner product using "kernel" functions that provide something like an inner product:

$$f(x) = b + \sum_{i=1}^n y_i \alpha_i k(x, x_i)$$


---
class: split-50

## Non-linear Support Vector Machine

.column[But, what is $k$? Let's consider two examples.

- _Polynomial kernel_: $k(x,x_i) = 1+\langle x, x_i \rangle^d$

- _RBF (radial) kernel_: $k(x,x_i) = \exp\{-\gamma \sum_{j=1}^p (x_{j}-x_{ij})^2\}$
]

.column[![](images/9_9.png)]

---

## Non-linear Support Vector Machine

```{r, echo=FALSE, fig.align='center'}
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))

k <- function(x, x0=0, gamma=1) {
  exp(-gamma*(x-x0)^2)
}
x <- seq(-3, 3, len=100)
plot(x, k(x), type="l", lwd=2, col=1, main="RBF kernel")
lines(x, k(x,gamma=10), lwd=2, col=2)
lines(x, k(x,gamma=.1), lwd=2, col=3)
legend("topright", legend=paste("gamma=",c(1,10,.1)), lty=1, lwd=2, col=1:3)
```

---

## Non-Linear Support Vector Machine

_Quiz_ Show that using $d=2$ with a polynomial kernel is equivalent to using a quadratic transformation on the input features

---

## Non-linear Support Vector Machine

The optimization problem is very similar

$$\mathrm{max}_{\alpha} \; \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k k(x_i,x_k) \\\\
\mathrm{s.t.} \; 0 \leq \alpha_i \leq \frac{1}{\lambda} \, \forall i$$

---

## SVM classification example

Let's try fitting SVMs to the credit card default dataset we saw in previous examples. 

Let's start with a linear SVM (where $k$ is the inner product). 

---

## SVM classification example

Here we are fitting three different SVMs resulting from using three different values of cost parameter $C=\frac{1}{\lambda}$.

```{r ch06_fitsvm, cache=TRUE, echo=FALSE}
library(e1071)
library(ISLR)
data(Default)

n <- nrow(Default)
train_indices <- sample(n, n/2)

costs <- c(.01, 1, 100)
svm_fits <- lapply(costs, function(cost) {
  svm(default~., data=Default, cost=cost, kernel="linear",subset=train_indices)
})
```

```{r, echo=FALSE}
number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Default[train_indices,])
  train <- mean(yhat != Default$default[train_indices])
  yhat <- predict(fit, newdata=Default[-train_indices,])
  test <- mean(yhat != Default$default[-train_indices])
  c(train=train, test=test)
})

tab <- data.frame(cost=costs, number_svs=number_svs, train_error=error_rate["train",]*100,test_error=error_rate["test",]*100)
knitr::kable(tab, format="html")
```

---

## SVM classification example

Let's try now a _non-linear_ SVM by using a radial kernel. 

Notice now that we have two parameters to provide to the fitting function: cost parameter $C$ and parameter $\gamma$ of the radial kernel function.

---

## SVM classification example

```{r ch06_fitradialsvm, cache=TRUE, echo=FALSE}
costs <- c(.01, 1, 10)
gamma <- c(.01, 1, 10)
parameters <- expand.grid(costs, gamma)

svm_fits <- lapply(seq(nrow(parameters)), function(i) {
  svm(default~., data=Default, cost=parameters[i,1], kernel="radial", gamma=parameters[i,2], subset=train_indices)
})
```


```{r, echo=FALSE}
number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Default[train_indices,])
  train <- mean(yhat != Default$default[train_indices])
  yhat <- predict(fit, newdata=Default[-train_indices,])
  test <- mean(yhat != Default$default[-train_indices])
  c(train=train, test=test)
})

tab <- data.frame(cost=parameters[,1], gamma=parameters[,2], number_svs=number_svs, train_error=error_rate["train",]*100,test_error=error_rate["test",]*100)
knitr::kable(tab, format="html")
```

---

## The SVM as a regularized estimation method

Recall the regularized estimation formulation for the SVM:

$$\mathrm{min}_{b, \mathbf{w}} \; \sum_{i=1}^N (1-y_if_i)_+ + \frac{\lambda}{2} \|\mathbf{w}\|^2$$

If observation $x_i$ is on the proper side of the margin, 

then $y_if_i > 1$ and thus $(1-y_ifi)_+ = 0$. 

Otherwise, $(1-y_if_i)_+$ equals the signed distance to the margin for observation $x_i$.

---

## The SVM as a regularized estimation method

In the non-linear case we can write it as equivalent problem as

$$\mathrm{min}_{b, \mathbf{\alpha}} \; \sum_{i=1}^N (1-y_if_i)_+ + \frac{\lambda}{2} \mathbf{\alpha}'K\mathbf{\alpha}$$
---

## Kernelized Logistic Regression

We can use the same "loss + penalty" formulation to obtain a kernelized version of logistic regression:

$$\mathrm{min}_{\beta_0, \beta} \; \sum_{i=1}^N \log(1+e^{-y_if_i}) + \frac{\lambda}{2} \alpha'K\alpha$$

---

## Kernelized Logistic Regression

As before, function $f$ has a linear expansion in terms of the kernel function:

$$f(x) = b + \sum_{i=1}^N \alpha_i k(x,x_i)$$

--

Unlike the SVM, the logistic regression loss function does not tend to set $\alpha_i=0$ for correctly classified observations.

---
class: split-40

## Kernelized logistic regression

.column[The loss function in the first term of the formulation is called "Hinge-loss".

We can compare it with the likelihood function for logistic regression.] 


.column[
```{r, echo=FALSE, fig.width=6, fig.height=5, fig.align="center"}
library(RColorBrewer)

palette(brewer.pal(3, "Dark2"))

curve(pmax(1-x,0), from=-3, to=3, lwd=1.8, col=1, xlab=expression(y*f), ylab="Loss")
curve(log(1+exp(-x)), add=TRUE, lwd=1.8, col=2)
abline(h=0, lwd=1.5, lty=2)
legend("topright", legend=c("Hinge Loss", "Logistic"), col=1:2, lwd=1.5)
```
]

---

## Kernelized Logistic Regression

As before, function $f$ has a linear expansion in terms of the kernel function:

$$f(x) = b + \sum_{i=1}^N \alpha_i k(x,x_i)$$

Nonetheless, function $f$ retains the interpretation in logistic regression

$$f(x) = \frac{Pr(Y = +1| X=x)}{Pr(Y = -1 | X=x)}$$

---

## Kernelized Regression

In similar fashion, we could build non-linear regression models using the "kernel trick" by using least squares as the loss function when predicting continuous outcomes.

$$\min{b, \alpha} \; \sum_{i=1}^N (y_i - f_i)^2 + \frac{\lambda}{2} \alpha'K\alpha$$

---

## Kernelized Regression

Again, function $f$ has a linear expansion in terms of the kernel function

$$f(x) = b + \sum_{i=1}^N \alpha_i k(x_i, x)$$

---

## Kernelized Regression

This does not lead to sparse representations over a subset of observations like SVMs.

However, a different choice of loss function, similar to hinge loss, can lead to sparse representations.

---

## Support Vector Regression

Support Vector Regression refers to the "loss + penalty" formulation when $\epsilon$-insensitive loss is used:

$$V_{\epsilon}(r) = \begin{cases}
0 & \mathrm{if } |r| < \epsilon \\\\
|r| - \epsilon & \mathrm{otherwise}
\end{cases}$$

---
class: split-60

## Support Vector Regression

.column[We can compare $\epsilon$-insensitive loss to squared loss]

.column[
```{r, echo=FALSE, fig.height=5, fig.width=5}
curve(x^2, from=-4, to=4, col=2, lwd=2, xlab=expression(r), ylab="Loss", ylim=c(0,6))
curve(pmax(0,abs(x)-1.5), add=TRUE, lwd=2, col=1)
abline(h=0, lwd=1.5, lty=2)
legend("topright", legend=c(expression(paste(epsilon,"-insensitive")), "squared"), col=1:2, lty=1, lwd=1.5)
```
]

---
class: split-40

## Applications: Modeling labeled data sequences

.column[Consider the case where predictors for observations are structured as sequences. 

For instance, predictors correspond to some variable measured over time.]

.column[
![](images/ts.png)
]

---
class: split-40

## Modeling labeled data sequences

.column[In this case, each observation is represented by a time series

we want to discriminate between time series that belong to two different classes.]

.column[
![](images/ts.png)
]

---

## Modeling labeled data sequences

Using the results above, we could model this using a Support Vector Machine, providing a kernel that captures similarity between time series.

Some proposals for this include:

- Autoregressive kernels: Cuturi, Doucet (2011) https://arxiv.org/abs/1101.0673. 

The likelihod of a vector autoregressive model is used to create a similarity metric.

---

## Modeling labeled data sequences

Using the results above, we could model this using a Support Vector Machine, providing a kernel that captures similarity between time series.

Some proposals for this include:

- Dynamic Time Warping Kernel: Shimodaira (2002) https://papers.nips.cc/paper/2131-dynamic-time-alignment-kernel-in-support-vector-machine.pdf. 

A warping method is used to define distances between data series

---
class: split-40

## Modeling labeled data sequences

.column[- Reservoir Computing: Chen et al. http://dl.acm.org/citation.cfm?id=2487700. 

Reservoir state models are used to represent time series and derive kernels
]

.column[.image-90[![](images/reservoir_ts.png)]]

---

## Structured Output

The "loss + penalty" representation also allows additional flexibility in the types of outcomes that are predicted. 

For instance, consider the case where outcomes are numerical vectors 

$\mathbf{y_i}=(y_{i1},y_{i2},\ldots,y_{iT})$ for each observation

along with predictors $x_i$ as before. 

---

## Structured Output

In this case, we would use function $f$ to represent a vector as well:

$$f(x) =  \left\langle 
\alpha_{01} + \sum_{i=1}^N \alpha_{i1}k(x_i,x) \\\\
\alpha_{02} + \sum_{i=1}^N \alpha_{i2}k(x_i,x) \\\\
\vdots \\\\
\alpha_{0T} + \sum_{i=1}^N \alpha_{iT}k(x_i,x)
\right\rangle$$

---
exclude: true

## Structured Output

We can then use a matrix norm on residuals as a loss function

$$\min_{\alpha_0, \alpha} \sum_{i=1}^N \|R\|_F^2 + \frac{1}{2} \sum_{j=1}^T \alpha_j' K \alpha_j$$

- column $i$ of residual matrix $R_i=y_i - f(x_i)$ 

- $\|R\|_F^2 = R'R$ is the Frobenius norm of residual matrix $R$

- $\alpha_j$ is the vector of parameters for component $j$ of the model.

---
exclude: true

## Structured Output

This regression model is over-parameterized 

Is difficult to estimate

Does not capture any underlying structure in the outcome vector $Y$.

---
exclude: true

## Structured Output

Methods to address this shortcoming are generalized under the term "structured output learning". 

A good starting point is https://mitpress.mit.edu/books/predicting-structured-data

---

## Summary

The general "loss + penalty" formulation along with kernel methods are capable of capturing a wide array of learning applications.

--

A number of effective methods to represent similarities between data series, e.g., time series, as kernel functions allows the usage of this framework to those types of problems.

--

Structured output formulations are applicable to learn multivariate outcomes with dependency structure between the components of the outcomes.

